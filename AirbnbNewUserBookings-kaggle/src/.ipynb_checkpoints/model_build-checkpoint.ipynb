{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 213451 entries, 0 to 213450\n",
      "Data columns (total 16 columns):\n",
      " #   Column                   Non-Null Count   Dtype  \n",
      "---  ------                   --------------   -----  \n",
      " 0   id                       213451 non-null  object \n",
      " 1   date_account_created     213451 non-null  object \n",
      " 2   timestamp_first_active   213451 non-null  int64  \n",
      " 3   date_first_booking       88908 non-null   object \n",
      " 4   gender                   213451 non-null  object \n",
      " 5   age                      125461 non-null  float64\n",
      " 6   signup_method            213451 non-null  object \n",
      " 7   signup_flow              213451 non-null  int64  \n",
      " 8   language                 213451 non-null  object \n",
      " 9   affiliate_channel        213451 non-null  object \n",
      " 10  affiliate_provider       213451 non-null  object \n",
      " 11  first_affiliate_tracked  207386 non-null  object \n",
      " 12  signup_app               213451 non-null  object \n",
      " 13  first_device_type        213451 non-null  object \n",
      " 14  first_browser            213451 non-null  object \n",
      " 15  country_destination      213451 non-null  object \n",
      "dtypes: float64(1), int64(2), object(13)\n",
      "memory usage: 26.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# importing neccessary libraries\n",
    "# 导入所需要的包\n",
    "\n",
    "# 数据处理\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 对数据进行绘制图形分析\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# 处理时间序列信息\n",
    "import datetime\n",
    "from datetime import date\n",
    "import os\n",
    "\n",
    "\n",
    "# LabelEncoder 编码，特征工程处理\n",
    "# 使用 sklearn 集成的\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# 使用模型 XGBClassifier\n",
    "# 采用 xgboost 集成的\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "# 利用 sklearn 集成的一些评测指标和模型选择\n",
    "import sklearn as sk\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import *\n",
    "\n",
    "# 用于高效存储数据，读写高效，内存性能\n",
    "import pickle \n",
    "\n",
    "# 警告过滤器\n",
    "import warnings\n",
    "# # 警告过滤器用于控制警告的行为，如忽略匹配的警告\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from data_processing import xtrain_new, ytrain_new, dcg_score, ndcg_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "# 逻辑回归模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "# 逻辑回归模型\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "lr = LogisticRegression(C = 1.0, penalty='l2', multi_class='ovr')\n",
    "RANDOM_STATE = 2020  # 随机种子\n",
    "\n",
    "# k-fold cross validation（k-折叠交叉验证）\n",
    "kf = KFold(n_splits=5, random_state=RANDOM_STATE) # 分成 5 个组\n",
    "train_score = [] \n",
    "cv_score = []\n",
    "\n",
    "# select a k  (value how many y):\n",
    "k_ndcg = 3 \n",
    "# kf.split: Generate indices to split data into training and test set.\n",
    "for train_index, test_index in kf.split(xtrain_new, ytrain_new):\n",
    "    # 训练集数据分割为训练集和测试集，y是目标变量\n",
    "    X_train, X_test = xtrain_new[train_index, :], xtrain_new[test_index, :]\n",
    "    y_train, y_test = ytrain_new[train_index], ytrain_new[test_index]\n",
    "        \n",
    "    lr.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = lr.predict_proba(X_test)\n",
    "    train_ndcg_score = ndcg_score(y_train, lr.predict_proba(X_train), k = k_ndcg)\n",
    "    cv_ndcg_score = ndcg_score(y_test, y_pred, k=k_ndcg)\n",
    "    \n",
    "    train_score.append(train_ndcg_score)\n",
    "    cv_score.append(cv_ndcg_score)\n",
    "    \n",
    "\n",
    "print (\"\\nThe training score is: {}\".format(np.mean(train_score)))\n",
    "print (\"\\nThe cv score is: {}\".format(np.mean(cv_score)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning curve of logistic regression\n",
    "# 观察逻辑回归模型学习曲线的变化\n",
    "# 调整逻辑回归参数 iteration\n",
    "\n",
    "# set the iterations\n",
    "iteration = [1,5,10,15,20, 50, 100]\n",
    "\n",
    "kf = KFold(n_splits=3, random_state=RANDOM_STATE)\n",
    "\n",
    "train_score = []\n",
    "cv_score = []\n",
    "\n",
    "# select a k:\n",
    "k_ndcg = 5\n",
    "\n",
    "for i, item in enumerate(iteration): \n",
    "\n",
    "    lr = LogisticRegression(C=1.0, max_iter=item, tol=1e-5, solver='newton-cg', multi_class='ovr') \n",
    "    train_score_iter = []\n",
    "    cv_score_iter = []\n",
    "\n",
    "    for train_index, test_index in kf.split(xtrain_new, ytrain_new):\n",
    "        X_train, X_test = xtrain_new[train_index, :], xtrain_new[test_index, :]\n",
    "        y_train, y_test = ytrain_new[train_index], ytrain_new[test_index]\n",
    "       \n",
    "        lr.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = lr.predict_proba(X_test)\n",
    "        train_ndcg_score = ndcg_score(y_train, lr.predict_proba(X_train), k = k_ndcg)\n",
    "        cv_ndcg_score = ndcg_score(y_test, y_pred, k=k_ndcg)\n",
    "\n",
    "        \n",
    "        train_score_iter.append(train_ndcg_score)\n",
    "        cv_score_iter.append(cv_ndcg_score)\n",
    "        \n",
    "    train_score.append(np.mean(train_score_iter))\n",
    "    cv_score.append(np.mean(cv_score_iter))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化分析\n",
    "# 结果：随着 iteration 的增大，逻辑回归模型的评分在不断升高，当 iteration 超过 20 的时候，模型的评分基本不变\n",
    "\n",
    "ymin = np.min(cv_score)-0.05\n",
    "ymax = np.max(train_score)+0.05\n",
    "\n",
    "plt.figure(figsize=(9,4))\n",
    "plt.plot(iteration, train_score, 'ro-', label = 'training')\n",
    "plt.plot(iteration, cv_score, 'b*-', label = 'Cross-validation')\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xlim(-5, np.max(iteration)+10)\n",
    "plt.ylim(ymin, ymax)\n",
    "plt.plot(np.linspace(20,20,50), np.linspace(ymin, ymax, 50), 'g--')\n",
    "plt.legend(loc = 'lower right', fontsize = 12)\n",
    "plt.title(\"Score vs iteration learning curve\")\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 改变数据量大小，查看效果分析\n",
    "# Chaning the sampling size\n",
    "# set the iter to the best iteration: iter = 20\n",
    "\n",
    "perc = [0.01,0.02,0.05,0.1,0.2,0.5,1]\n",
    "\n",
    "kf = KFold(n_splits=3, random_state=RANDOM_STATE)\n",
    "\n",
    "train_score = []\n",
    "cv_score = []\n",
    "\n",
    "# select a k:\n",
    "k_ndcg = 5\n",
    "\n",
    "for i, item in enumerate(perc):\n",
    "    \n",
    "    lr = LogisticRegression(C=1.0, max_iter=20, tol=1e-6, solver='newton-cg', multi_class='ovr')\n",
    "    train_score_iter = []\n",
    "    cv_score_iter = []\n",
    "    \n",
    "    n = int(xtrain_new.shape[0]*item)\n",
    "    xtrain_perc = xtrain_new[:n, :]\n",
    "    ytrain_perc = ytrain_new[:n]\n",
    "\n",
    "\n",
    "    for train_index, test_index in kf.split(xtrain_perc, ytrain_perc):\n",
    "        \n",
    "        X_train, X_test = xtrain_perc[train_index, :], xtrain_perc[test_index, :]\n",
    "        y_train, y_test = ytrain_perc[train_index], ytrain_perc[test_index]\n",
    "\n",
    "        print(X_train.shape, X_test.shape)\n",
    "        \n",
    "        lr.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = lr.predict_proba(X_test)\n",
    "        train_ndcg_score = ndcg_score(y_train, lr.predict_proba(X_train), k = k_ndcg)\n",
    "        cv_ndcg_score = ndcg_score(y_test, y_pred, k=k_ndcg)\n",
    "\n",
    "        train_score_iter.append(train_ndcg_score)\n",
    "        cv_score_iter.append(cv_ndcg_score)\n",
    "        \n",
    "    train_score.append(np.mean(train_score_iter))\n",
    "    cv_score.append(np.mean(cv_score_iter))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化分析\n",
    "# 结果：随着数据量的增加，逻辑回归模型对测试集的预测评分（蓝色线）在不断上升，\n",
    "#     因为在训练模型时只用了 10% 的数据，如果使用全部的数据，效果可能会更好\n",
    "\n",
    "ymin = np.min(cv_score)-0.1\n",
    "ymax = np.max(train_score)+0.1\n",
    "\n",
    "plt.figure(figsize=(9,4))\n",
    "plt.plot(np.array(perc)*100, train_score, 'ro-', label = 'training')\n",
    "plt.plot(np.array(perc)*100, cv_score, 'bo-', label = 'Cross-validation')\n",
    "plt.xlabel(\"Sample size (unit %)\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xlim(-5, np.max(perc)*100+10)\n",
    "plt.ylim(ymin, ymax)\n",
    "\n",
    "plt.legend(loc = 'lower right', fontsize = 12)\n",
    "plt.title(\"Score vs sample size learning curve\")\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 树模型\n",
    "# 其中的模型包括 DecisionTree，RandomForest，AdaBoost，Bagging，ExtraTree，GraBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 树模型\n",
    "# 其中的模型包括 DecisionTree，RandomForest，AdaBoost，Bagging，ExtraTree，GraBoost\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "LEARNING_RATE = 0.1\n",
    "N_ESTIMATORS = 50\n",
    "RANDOM_STATE = 2017\n",
    "MAX_DEPTH = 9\n",
    "\n",
    "# 建了一个 tree 字典\n",
    "clf_tree ={\n",
    "    'DTree': DecisionTreeClassifier(max_depth=MAX_DEPTH,\n",
    "                                    random_state=RANDOM_STATE),\n",
    "    \n",
    "    'RF': RandomForestClassifier(n_estimators=N_ESTIMATORS,\n",
    "                                 max_depth=MAX_DEPTH,\n",
    "                                 random_state=RANDOM_STATE),\n",
    "    \n",
    "    'AdaBoost': AdaBoostClassifier(n_estimators=N_ESTIMATORS,\n",
    "                                   learning_rate=LEARNING_RATE,\n",
    "                                   random_state=RANDOM_STATE),\n",
    "    \n",
    "    'Bagging': BaggingClassifier(n_estimators=N_ESTIMATORS,\n",
    "                                 random_state=RANDOM_STATE),\n",
    "    \n",
    "    'ExtraTree': ExtraTreesClassifier(max_depth=MAX_DEPTH,\n",
    "                                      n_estimators=N_ESTIMATORS,\n",
    "                                      random_state=RANDOM_STATE),\n",
    "    \n",
    "    'GraBoost': GradientBoostingClassifier(learning_rate=LEARNING_RATE,\n",
    "                                           max_depth=MAX_DEPTH,\n",
    "                                           n_estimators=N_ESTIMATORS,\n",
    "                                           random_state=RANDOM_STATE)\n",
    "}\n",
    "train_score = []\n",
    "cv_score = []\n",
    "\n",
    "kf = KFold(n_splits=3, random_state=RANDOM_STATE)\n",
    "\n",
    "k_ndcg = 5\n",
    "\n",
    "for key in clf_tree.keys():\n",
    "    \n",
    "    clf = clf_tree.get(key)\n",
    "    \n",
    "    train_score_iter = []\n",
    "    cv_score_iter = []\n",
    "\n",
    "    for train_index, test_index in kf.split(xtrain_new, ytrain_new):\n",
    "\n",
    "        X_train, X_test = xtrain_new[train_index, :], xtrain_new[test_index, :]\n",
    "        y_train, y_test = ytrain_new[train_index], ytrain_new[test_index]\n",
    "        \n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = clf.predict_proba(X_test)\n",
    "        train_ndcg_score = ndcg_score(y_train, clf.predict_proba(X_train), k = k_ndcg)\n",
    "        cv_ndcg_score = ndcg_score(y_test, y_pred, k=k_ndcg)\n",
    "\n",
    "        train_score_iter.append(train_ndcg_score)\n",
    "        cv_score_iter.append(cv_ndcg_score)\n",
    "        \n",
    "    train_score.append(np.mean(train_score_iter))\n",
    "    cv_score.append(np.mean(cv_score_iter))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化分析\n",
    "\n",
    "train_score_tree = train_score\n",
    "cv_score_tree = cv_score\n",
    "\n",
    "ymin = np.min(cv_score)-0.05\n",
    "ymax = np.max(train_score)+0.05\n",
    "\n",
    "x_ticks = clf_tree.keys()\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(range(len(x_ticks)), train_score_tree, 'ro-', label = 'training')\n",
    "plt.plot(range(len(x_ticks)),cv_score_tree, 'bo-', label = 'Cross-validation')\n",
    "\n",
    "plt.xticks(range(len(x_ticks)),x_ticks,rotation = 45, fontsize = 10)\n",
    "plt.xlabel(\"Tree method\", fontsize = 12)\n",
    "plt.ylabel(\"Score\", fontsize = 12)\n",
    "plt.xlim(-0.5, 5.5)\n",
    "plt.ylim(ymin, ymax)\n",
    "\n",
    "plt.legend(loc = 'best', fontsize = 12)\n",
    "plt.title(\"Different tree methods\")\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM模型\n",
    "# 根据核函数的不同，又分为：SVM-rbf，SVM-poly，SVM-linear等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM模型\n",
    "# 根据核函数的不同，又分为：SVM-rbf，SVM-poly，SVM-linear等\n",
    "\n",
    "TOL = 1e-4\n",
    "MAX_ITER = 1000\n",
    "\n",
    "clf_svm = {\n",
    "    \n",
    "    'SVM-rbf': SVC(kernel='rbf',\n",
    "                   max_iter=MAX_ITER,\n",
    "                   tol=TOL, random_state=RANDOM_STATE,\n",
    "                   decision_function_shape='ovr'),     \n",
    "    \n",
    "    'SVM-poly': SVC(kernel='poly',\n",
    "                   max_iter=MAX_ITER,\n",
    "                   tol=TOL, random_state=RANDOM_STATE,\n",
    "                   decision_function_shape='ovr'),     \n",
    "    \n",
    "    'SVM-linear': SVC(kernel='linear',\n",
    "                      max_iter=MAX_ITER,\n",
    "                      tol=TOL, \n",
    "                      random_state=RANDOM_STATE,\n",
    "                      decision_function_shape='ovr'),  \n",
    "    \n",
    "    'LinearSVC': LinearSVC(max_iter=MAX_ITER,\n",
    "                            tol=TOL,\n",
    "                            random_state=RANDOM_STATE,\n",
    "                            multi_class = 'ovr')  \n",
    "                            \n",
    "}     \n",
    "\n",
    "train_score_svm = []\n",
    "cv_score_svm = []\n",
    "\n",
    "kf = KFold(n_splits=3, random_state=RANDOM_STATE)\n",
    "\n",
    "k_ndcg = 5\n",
    "\n",
    "for key in clf_svm.keys():\n",
    "    \n",
    "    clf = clf_svm.get(key)\n",
    "\n",
    "    train_score_iter = []\n",
    "    cv_score_iter = []\n",
    "\n",
    "    for train_index, test_index in kf.split(xtrain_new, ytrain_new):\n",
    "\n",
    "        X_train, X_test = xtrain_new[train_index, :], xtrain_new[test_index, :]\n",
    "        y_train, y_test = ytrain_new[train_index], ytrain_new[test_index]\n",
    "        \n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = clf.decision_function(X_test)\n",
    "        train_ndcg_score = ndcg_score(y_train, clf.decision_function(X_train), k = k_ndcg)\n",
    "        cv_ndcg_score = ndcg_score(y_test, y_pred, k=k_ndcg)\n",
    "\n",
    "        train_score_iter.append(train_ndcg_score)\n",
    "        cv_score_iter.append(cv_ndcg_score)\n",
    "        \n",
    "    train_score_svm.append(np.mean(train_score_iter))\n",
    "    cv_score_svm.append(np.mean(cv_score_iter))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化分析\n",
    "\n",
    "ymin = np.min(cv_score_svm)-0.05\n",
    "ymax = np.max(train_score_svm)+0.05\n",
    "\n",
    "x_ticks = clf_svm.keys()\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(range(len(x_ticks)), train_score_svm, 'ro-', label = 'training')\n",
    "plt.plot(range(len(x_ticks)),cv_score_svm, 'bo-', label = 'Cross-validation')\n",
    "\n",
    "plt.xticks(range(len(x_ticks)),x_ticks,rotation = 45, fontsize = 10)\n",
    "plt.xlabel(\"Tree method\", fontsize = 12)\n",
    "plt.ylabel(\"Score\", fontsize = 12)\n",
    "plt.xlim(-0.5, 3.5)\n",
    "plt.ylim(ymin, ymax)\n",
    "\n",
    "plt.legend(loc = 'best', fontsize = 12)\n",
    "plt.title(\"Different SVM methods\")\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# xgboost\n",
    "# kaggle 比赛中常用的一个模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost\n",
    "# kaggle 比赛中常用的一个模型\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "def customized_eval(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    top = []\n",
    "    for i in range(preds.shape[0]):\n",
    "        top.append(np.argsort(preds[i])[::-1][:5])\n",
    "    mat = np.reshape(np.repeat(labels,np.shape(top)[1]) == np.array(top).ravel(),np.array(top).shape).astype(int)\n",
    "    score = np.mean(np.sum(mat/np.log2(np.arange(2, mat.shape[1] + 2)),axis = 1))\n",
    "    return 'ndcg5', score\n",
    "# xgboost parameters\n",
    "\n",
    "NUM_XGB = 200\n",
    "\n",
    "params = {}\n",
    "params['colsample_bytree'] = 0.6\n",
    "params['max_depth'] = 6\n",
    "params['subsample'] = 0.8\n",
    "params['eta'] = 0.3\n",
    "params['seed'] = RANDOM_STATE\n",
    "params['num_class'] = 12\n",
    "params['objective'] = 'multi:softprob'   # output the probability instead of class. \n",
    "train_score_iter = []\n",
    "cv_score_iter = []\n",
    "\n",
    "kf = KFold(n_splits = 3, random_state=RANDOM_STATE)\n",
    "\n",
    "k_ndcg = 5\n",
    "\n",
    "for train_index, test_index in kf.split(xtrain_new, ytrain_new):\n",
    "\n",
    "    X_train, X_test = xtrain_new[train_index, :], xtrain_new[test_index, :]\n",
    "    y_train, y_test = ytrain_new[train_index], ytrain_new[test_index]\n",
    "    \n",
    "    train_xgb = xgb.DMatrix(X_train, label= y_train)\n",
    "    test_xgb = xgb.DMatrix(X_test, label = y_test)\n",
    "    \n",
    "    watchlist = [ (train_xgb,'train'), (test_xgb, 'test') ]\n",
    "\n",
    "    bst = xgb.train(params, \n",
    "                     train_xgb,\n",
    "                     NUM_XGB,\n",
    "                     watchlist,\n",
    "                     feval = customized_eval,\n",
    "                     verbose_eval = 3,\n",
    "                     early_stopping_rounds = 5)\n",
    "    \n",
    "    \n",
    "    #bst = xgb.train( params, dtrain, num_round, evallist )\n",
    "\n",
    "    y_pred = np.array(bst.predict(test_xgb))\n",
    "    y_pred_train = np.array(bst.predict(train_xgb))\n",
    "    train_ndcg_score = ndcg_score(y_train, y_pred_train , k = k_ndcg)\n",
    "    cv_ndcg_score = ndcg_score(y_test, y_pred, k=k_ndcg)\n",
    "\n",
    "    train_score_iter.append(train_ndcg_score)\n",
    "    cv_score_iter.append(cv_ndcg_score)\n",
    "\n",
    "train_score_xgb = np.mean(train_score_iter)\n",
    "cv_score_xgb = np.mean(cv_score_iter)\n",
    "\n",
    "print (\"\\nThe training score is: {}\".format(train_score_xgb))\n",
    "print (\"The cv score is: {}\\n\".format(cv_score_xgb))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型比较"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型评估比较\n",
    "\n",
    "model_cvscore = np.hstack((cv_score_lr, cv_score_tree, cv_score_svm, cv_score_xgb))\n",
    "model_name = np.array(['LinearReg','ExtraTree','DTree','RF','GraBoost','Bagging','AdaBoost','LinearSVC','SVM-linear','SVM-rbf','SVM-poly','Xgboost'])\n",
    "fig = plt.figure(figsize=(8,4))\n",
    "\n",
    "sns.barplot(model_cvscore, model_name, palette=\"Blues_d\")\n",
    "\n",
    "plt.xticks(rotation=0, size = 10)\n",
    "plt.xlabel(\"CV score\", fontsize = 12)\n",
    "plt.ylabel(\"Model\", fontsize = 12)\n",
    "plt.title(\"Cross-validation score for different models\")\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 总结\n",
    "\n",
    "# 对数据的理解和探索很重要，数据的处理以及清洗，思路和方法一样重要\n",
    "# 可以通过特征工程，进一步提取特征，使得原始数据特征更易于模型构建和学习\n",
    "# 模型评估的方法有很多种，选取适宜的模型评估方法\n",
    "# 目前只用了 10% 的数据进行模型训练，用全部的数据集进行训练，效果可能会更好\n",
    "# 需要深入理解模型算法，学会怎么调参，优化模型参数\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
